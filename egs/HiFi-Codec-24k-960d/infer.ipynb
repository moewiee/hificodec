{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../')\n",
    "import json\n",
    "import torchaudio\n",
    "import os\n",
    "import librosa\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "import torch\n",
    "from academicodec.models.hificodec.vqvae import VQVAE\n",
    "from academicodec.models.hificodec.meldataset import mel_spectrogram\n",
    "from librosa.util import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_config_path = './config_24k_960d.json'\n",
    "refiner_config_path = './config_refiner_24k_120d.json'\n",
    "\n",
    "pretrained_path = '50m_logs_4cb_960d/step_110k'\n",
    "# pretrained_path = '50m_logs_2cb_960d/g_00078000'\n",
    "refiner_path = '10m_logs_refiner_64cb_240d/g_00015000'\n",
    "\n",
    "wav_path = './sample.wav'\n",
    "base_output_path = './base.wav'\n",
    "refined_output_path = './refined.wav'\n",
    "\n",
    "assert pretrained_config_path and os.path.exists(pretrained_config_path)\n",
    "assert refiner_config_path and os.path.exists(refiner_config_path)\n",
    "assert pretrained_path and os.path.exists(pretrained_path)\n",
    "if refiner_path:\n",
    "    assert os.path.exists(refiner_path)\n",
    "assert wav_path and os.path.exists(wav_path)\n",
    "if refined_output_path:\n",
    "    assert refiner_path and os.path.exists(refiner_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pretrained_config_path, 'r') as f:\n",
    "    pretrained_config = json.load(f)\n",
    "\n",
    "with open(refiner_config_path, 'r') as f:\n",
    "    refiner_config = json.load(f)\n",
    "    sample_rate = refiner_config['sampling_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav.shape: (240000,)\n",
      "torch.Size([1, 240000])\n"
     ]
    }
   ],
   "source": [
    "wav, sr = librosa.load(wav_path, sr=sample_rate)\n",
    "print(\"wav.shape:\",wav.shape)\n",
    "assert sr == sample_rate\n",
    "\n",
    "wav = normalize(wav) * 0.95\n",
    "wav = torch.FloatTensor(wav).unsqueeze(0)\n",
    "\n",
    "print(wav.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model and load weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "print(\"Init model and load weights\")\n",
    "\n",
    "pretrained_model = VQVAE(\n",
    "    pretrained_config_path,\n",
    "    ckpt_path=pretrained_path,\n",
    "    with_encoder=True)\n",
    "pretrained_model.eval()\n",
    "\n",
    "refiner_model = VQVAE(\n",
    "    refiner_config_path,\n",
    "    ckpt_path=refiner_path,\n",
    "    with_encoder=True\n",
    ")\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_targets = pretrained_model.wav_to_text_target(wav)\n",
    "y_hat = pretrained_model.text_target_to_wav(text_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat.size()\n",
    "torchaudio.save('text_target_and_back.wav', y_hat[0], sample_rate, channels_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 250])\n"
     ]
    }
   ],
   "source": [
    "latent_image = pretrained_model.text_target_to_latent_image(text_targets)\n",
    "print(latent_image.size())\n",
    "y_hat = pretrained_model.generator(latent_image)\n",
    "torchaudio.save('text_target_and_back.wav', y_hat[0], sample_rate, channels_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_token = pretrained_model.wav_to_acoustic_token(wav)\n",
    "generated_output = pretrained_model.acoustic_token_to_wav(acoustic_token)\n",
    "torchaudio.save('debug.wav', generated_output[0], sample_rate, channels_first=True)\n",
    "\n",
    "refined_generated_output = refiner_model(generated_output[0])\n",
    "torchaudio.save('debug_refined.wav', generated_output[0], sample_rate, channels_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if base_output_path:\n",
    "#     torchaudio.save(base_output_path, base_y_hat[0], sample_rate, channels_first=True)\n",
    "# if refined_output_path:\n",
    "#     torchaudio.save(refined_output_path, refined_y_hat[0], sample_rate, channels_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.nn.functional.mse_loss(mel_spectrogram(\n",
    "#     wav.squeeze(1).cpu(), 1024, 80,\n",
    "#     24000, 256, 1024,\n",
    "#     0, 8000), mel_spectrogram(\n",
    "#     base_y_hat.squeeze(1).detach(), 1024, 80,\n",
    "#     24000, 256, 1024,\n",
    "#     0, 8000)).numpy())\n",
    "\n",
    "# print(torch.nn.functional.l1_loss(mel_spectrogram(\n",
    "#     wav.squeeze(1).cpu(), 1024, 80,\n",
    "#     24000, 256, 1024,\n",
    "#     0, 8000), mel_spectrogram(\n",
    "#     base_y_hat.squeeze(1).detach(), 1024, 80,\n",
    "#     24000, 256, 1024,\n",
    "#     0, 8000)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if refiner_model:\n",
    "#     print(torch.nn.functional.mse_loss(mel_spectrogram(\n",
    "#         wav.squeeze(1).cpu(), 1024, 80,\n",
    "#         24000, 256, 1024,\n",
    "#         0, 8000), mel_spectrogram(\n",
    "#         refined_y_hat.squeeze(1).detach(), 1024, 80,\n",
    "#         24000, 256, 1024,\n",
    "#         0, 8000)).numpy())\n",
    "\n",
    "#     print(torch.nn.functional.l1_loss(mel_spectrogram(\n",
    "#         wav.squeeze(1).cpu(), 1024, 80,\n",
    "#         24000, 256, 1024,\n",
    "#         0, 8000), mel_spectrogram(\n",
    "#         refined_y_hat.squeeze(1).detach(), 1024, 80,\n",
    "#         24000, 256, 1024,\n",
    "#         0, 8000)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
